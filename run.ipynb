{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BhPKd5nleLrv",
        "outputId": "c85246b7-9741-4338-da0c-ee40d2285ad5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://mirrors.aliyun.com/pypi/simple/, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading https://mirrors.aliyun.com/pypi/packages/bb/23/76fe2c3976185325021c8ff6ae72051bb4302e3f0d89d1986cc877505b4c/transformers-4.20.1-py3-none-any.whl (4.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.4 MB 534 kB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading https://mirrors.aliyun.com/pypi/packages/eb/5f/6e6fe6904e1a9c67bc2ca5629a69e7a5a0b17f079da838bab98a1e548b25/PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 555 kB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading https://mirrors.aliyun.com/pypi/packages/84/aa/f0509ce47f22334654b3bbe9553525160df8ad7a02a37f6eab27def3c4c2/tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 428 kB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.4)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading https://mirrors.aliyun.com/pypi/packages/d8/2c/9af8451ab780598e3b26a84d4f0e3844841456657401eb6843fdb622bb41/huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n",
            "\u001b[K     |████████████████████████████████| 101 kB 547 kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.8.1 pyyaml-6.0 tokenizers-0.12.1 transformers-4.20.1\n",
            "Looking in indexes: https://mirrors.aliyun.com/pypi/simple/, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.3.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.21.6)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2022.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
            "Looking in indexes: https://mirrors.aliyun.com/pypi/simple/, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.21.6)\n",
            "Looking in indexes: https://mirrors.aliyun.com/pypi/simple/, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.11.0+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (4.1.1)\n",
            "Looking in indexes: https://mirrors.aliyun.com/pypi/simple/, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (3.2.2)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.21.6)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.4.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib) (4.1.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers -i https://mirrors.aliyun.com/pypi/simple/\n",
        "!pip install pandas -i https://mirrors.aliyun.com/pypi/simple/\n",
        "!pip install numpy -i https://mirrors.aliyun.com/pypi/simple/\n",
        "!pip install torch -i https://mirrors.aliyun.com/pypi/simple/\n",
        "!pip install matplotlib -i https://mirrors.aliyun.com/pypi/simple/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8CeIgnWjeij6",
        "outputId": "09a0faa2-9f6e-4a23-928e-61ad23f3f350"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://mirrors.aliyun.com/pypi/simple/, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (0.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn) (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.21.6)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (3.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.1.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install sklearn -i https://mirrors.aliyun.com/pypi/simple/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iXgxNQs0e3DX"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as functional\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "import gc\n",
        "from transformers import BertModel\n",
        "from sklearn.metrics import roc_auc_score,f1_score\n",
        "import time\n",
        "import datetime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_1Pcnjcf0UV"
      },
      "source": [
        "# 1. **加载数据集，启动GPU**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8WlA2J2fzez",
        "outputId": "8ebc99c6-ed81-4e1b-a7a2-6b787fbad7d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j0bcGu9zgFez",
        "outputId": "a3cc3d96-413a-4f41-e78f-ed899a989a76"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using GPU: True\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(0)\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device('cuda' if use_cuda else 'cpu')\n",
        "if use_cuda:\n",
        "    torch.cuda.manual_seed(0)\n",
        "    \n",
        "print(\"Using GPU: {}\".format(use_cuda))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8b-jd9PDgRV3"
      },
      "outputs": [],
      "source": [
        "train = pd.read_csv('/content/drive/MyDrive/covid19_data/Constraint_Train.csv')\n",
        "val = pd.read_csv('/content/drive/MyDrive/covid19_data/Constraint_Val.csv')\n",
        "train[\"label\"] = train[\"label\"].map({\"real\": 1, \"fake\": 0})\n",
        "val[\"label\"] = val[\"label\"].map({\"real\": 1, \"fake\": 0})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "h-DGpeOGgXO_",
        "outputId": "444c373e-3996-44a8-fe25-dd9a8aca637b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-f3f61f23-d1a7-4c78-85a0-9e8eeac86f3f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>tweet</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>The CDC currently reports 99031 deaths. In gen...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>States reported 1121 deaths a small rise from ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>Politically Correct Woman (Almost) Uses Pandem...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>#IndiaFightsCorona: We have 1524 #COVID testin...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>Populous states can generate large case counts...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6415</th>\n",
              "      <td>6416</td>\n",
              "      <td>A tiger tested positive for COVID-19 please st...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6416</th>\n",
              "      <td>6417</td>\n",
              "      <td>???Autopsies prove that COVID-19 is??� a blood...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6417</th>\n",
              "      <td>6418</td>\n",
              "      <td>_A post claims a COVID-19 vaccine has already ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6418</th>\n",
              "      <td>6419</td>\n",
              "      <td>Aamir Khan Donate 250 Cr. In PM Relief Cares Fund</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6419</th>\n",
              "      <td>6420</td>\n",
              "      <td>It has been 93 days since the last case of COV...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6420 rows × 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f3f61f23-d1a7-4c78-85a0-9e8eeac86f3f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f3f61f23-d1a7-4c78-85a0-9e8eeac86f3f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f3f61f23-d1a7-4c78-85a0-9e8eeac86f3f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "        id                                              tweet  label\n",
              "0        1  The CDC currently reports 99031 deaths. In gen...      1\n",
              "1        2  States reported 1121 deaths a small rise from ...      1\n",
              "2        3  Politically Correct Woman (Almost) Uses Pandem...      0\n",
              "3        4  #IndiaFightsCorona: We have 1524 #COVID testin...      1\n",
              "4        5  Populous states can generate large case counts...      1\n",
              "...    ...                                                ...    ...\n",
              "6415  6416  A tiger tested positive for COVID-19 please st...      0\n",
              "6416  6417  ???Autopsies prove that COVID-19 is??� a blood...      0\n",
              "6417  6418  _A post claims a COVID-19 vaccine has already ...      0\n",
              "6418  6419  Aamir Khan Donate 250 Cr. In PM Relief Cares Fund      0\n",
              "6419  6420  It has been 93 days since the last case of COV...      1\n",
              "\n",
              "[6420 rows x 3 columns]"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "tEI6yZ4_iEPM",
        "outputId": "c6025002-2d38-4785-8e94-8513d6e8f4a2"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-652d8673-a9d2-40a7-bdbd-af11de28351e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>tweet</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Chinese converting to Islam after realising th...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>11 out of 13 people (from the Diamond Princess...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>COVID-19 Is Caused By A Bacterium, Not Virus A...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>Mike Pence in RNC speech praises Donald Trump’...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>6/10 Sky's @EdConwaySky explains the latest #C...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2135</th>\n",
              "      <td>2136</td>\n",
              "      <td>Donald Trump wrongly claimed that New Zealand ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2136</th>\n",
              "      <td>2137</td>\n",
              "      <td>Current understanding is #COVID19 spreads most...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2137</th>\n",
              "      <td>2138</td>\n",
              "      <td>Nothing screams “I am sat around doing fuck al...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2138</th>\n",
              "      <td>2139</td>\n",
              "      <td>Birx says COVID-19 outbreak not under control ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2139</th>\n",
              "      <td>2140</td>\n",
              "      <td>Another 4422 new coronavirus cases have been c...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2140 rows × 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-652d8673-a9d2-40a7-bdbd-af11de28351e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-652d8673-a9d2-40a7-bdbd-af11de28351e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-652d8673-a9d2-40a7-bdbd-af11de28351e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "        id                                              tweet  label\n",
              "0        1  Chinese converting to Islam after realising th...      0\n",
              "1        2  11 out of 13 people (from the Diamond Princess...      0\n",
              "2        3  COVID-19 Is Caused By A Bacterium, Not Virus A...      0\n",
              "3        4  Mike Pence in RNC speech praises Donald Trump’...      0\n",
              "4        5  6/10 Sky's @EdConwaySky explains the latest #C...      1\n",
              "...    ...                                                ...    ...\n",
              "2135  2136  Donald Trump wrongly claimed that New Zealand ...      0\n",
              "2136  2137  Current understanding is #COVID19 spreads most...      1\n",
              "2137  2138  Nothing screams “I am sat around doing fuck al...      0\n",
              "2138  2139  Birx says COVID-19 outbreak not under control ...      0\n",
              "2139  2140  Another 4422 new coronavirus cases have been c...      1\n",
              "\n",
              "[2140 rows x 3 columns]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "XRMVUUFeiLID",
        "outputId": "9065c66b-998f-49b4-e336-ca1bce1d564e"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-9b94acf2-24c5-4342-95a4-38d79a6a141b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>The CDC currently reports 99031 deaths. In gen...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>States reported 1121 deaths a small rise from ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Politically Correct Woman (Almost) Uses Pandem...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>#IndiaFightsCorona: We have 1524 #COVID testin...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Populous states can generate large case counts...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8555</th>\n",
              "      <td>Donald Trump wrongly claimed that New Zealand ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8556</th>\n",
              "      <td>Current understanding is #COVID19 spreads most...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8557</th>\n",
              "      <td>Nothing screams “I am sat around doing fuck al...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8558</th>\n",
              "      <td>Birx says COVID-19 outbreak not under control ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8559</th>\n",
              "      <td>Another 4422 new coronavirus cases have been c...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8560 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9b94acf2-24c5-4342-95a4-38d79a6a141b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9b94acf2-24c5-4342-95a4-38d79a6a141b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9b94acf2-24c5-4342-95a4-38d79a6a141b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                                  tweet  label\n",
              "0     The CDC currently reports 99031 deaths. In gen...      1\n",
              "1     States reported 1121 deaths a small rise from ...      1\n",
              "2     Politically Correct Woman (Almost) Uses Pandem...      0\n",
              "3     #IndiaFightsCorona: We have 1524 #COVID testin...      1\n",
              "4     Populous states can generate large case counts...      1\n",
              "...                                                 ...    ...\n",
              "8555  Donald Trump wrongly claimed that New Zealand ...      0\n",
              "8556  Current understanding is #COVID19 spreads most...      1\n",
              "8557  Nothing screams “I am sat around doing fuck al...      0\n",
              "8558  Birx says COVID-19 outbreak not under control ...      0\n",
              "8559  Another 4422 new coronavirus cases have been c...      1\n",
              "\n",
              "[8560 rows x 2 columns]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data = pd.concat([train, val], axis=0, ignore_index=True).drop([\"id\"], axis=1)\n",
        "\n",
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uN2_XYJIiez5"
      },
      "source": [
        "## 2.**Tokenize数据**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gaOJj3aHinCj"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer\n",
        "print('Loading BERT tokenizer...')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HwjCmb3mir5l"
      },
      "outputs": [],
      "source": [
        "tweets = data.tweet.values\n",
        "labels = data.label.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O0GdIE7yi8_J"
      },
      "outputs": [],
      "source": [
        "tweets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1N-_kdXxjKNm"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from string import punctuation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWE1QYZFjSl-"
      },
      "source": [
        "**数据预处理和数据探索分析**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Y-qSrhRjWzI"
      },
      "outputs": [],
      "source": [
        "def preprocess(data):\n",
        "    #remove url and hashtag\n",
        "    for i in range(data.shape[0]):\n",
        "        text=data[i].lower()\n",
        "        text1=''.join([word+\" \" for word in text.split()])\n",
        "        data[i]=text1\n",
        "    giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n",
        "        '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
        "    mention_regex = '@[\\w\\-]+'\n",
        "    hashtag_regex = '#[\\w\\-]+'\n",
        "    space_pattern = '\\s+'\n",
        "\n",
        "    for i in range(data.shape[0]):\n",
        "        text_string = data[i]\n",
        "        parsed_text = re.sub(hashtag_regex, '', text_string)\n",
        "        parsed_text = re.sub(giant_url_regex, '', parsed_text)\n",
        "        parsed_text = re.sub(mention_regex, '', parsed_text) \n",
        "        #remove punctuation\n",
        "        parsed_text = re.sub(r\"[{}]+\".format(punctuation), '', parsed_text) \n",
        "        parsed_text = re.sub(space_pattern, ' ', parsed_text)\n",
        "        data[i] = parsed_text\n",
        "    return data\n",
        "tweets = preprocess(tweets)\n",
        "print(tweets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVM-6ahoOxr8"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7RQtHDhzjgN-"
      },
      "outputs": [],
      "source": [
        "# 打印原始句子\n",
        "print(' Original: ', tweets[0])\n",
        "\n",
        "# 分词.\n",
        "print('Tokenized: ', tokenizer.tokenize(tweets[0]))\n",
        "\n",
        "# 将tokens映射为id.\n",
        "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(tweets[0])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yOYE_97Fj6fj"
      },
      "outputs": [],
      "source": [
        "max_len = 0\n",
        "ind = [100,200,300,400,500,512]\n",
        "for i in ind:\n",
        "  count = 0\n",
        "  for tweet in tweets:\n",
        "      max_len = max(max_len, len(tweet))\n",
        "      if len(tweet)>i:\n",
        "        count+=1\n",
        "  print(\"Count of sentence length over {} is: \".format(i), count)\n",
        "print('Max sentence length: ', max_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EnzAt2mEkEW1"
      },
      "outputs": [],
      "source": [
        "input_ids = []\n",
        "attention_masks = []\n",
        "for tweet in tweets:\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "    tweet,\n",
        "    add_special_tokens = True, # 增加分隔符号'[CLS]'和'[SEP]'\n",
        "    max_length = 512,# 最大句子长度，用于pad或者truncate句子\n",
        "    pad_to_max_length = True,\n",
        "    return_attention_mask = True,\n",
        "    return_tensors = 'pt',)\n",
        "  \n",
        "    # 向input_ids中加入encode后的句子    \n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    \n",
        "    # 向attention_masks加入掩码\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "# 转换为tensor\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "labels = torch.tensor(labels)\n",
        "\n",
        "# 打印第0个样本\n",
        "print('Original: ', tweets[0:3])\n",
        "print('Token IDs:', input_ids[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X9Yyb1sjlauD"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import TensorDataset, random_split\n",
        "\n",
        "# 设置数据集TensorDataset.\n",
        "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "\n",
        "# 分隔验证集与测试集\n",
        "train_size = int(0.9 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size],generator=torch.Generator().manual_seed(42))\n",
        "\n",
        "print('{:>5,} training samples'.format(train_size))\n",
        "print('{:>5,} validation samples'.format(val_size))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oUCT19D_l2V-"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "# 设置数据集加载器DataLoader，按batch_size分组\n",
        "batch_size = 16\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "            train_dataset,  \n",
        "            shuffle = True,\n",
        "            batch_size = batch_size \n",
        "        )\n",
        "\n",
        "validation_dataloader = DataLoader(\n",
        "            val_dataset,\n",
        "            shuffle = False,\n",
        "            batch_size = batch_size \n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3zBWxLEdmKds"
      },
      "outputs": [],
      "source": [
        "def format_time(elapsed):\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzt6QMSpmhO-"
      },
      "source": [
        "## 3.**训练与测试**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhlaLaEAmo_G"
      },
      "source": [
        "### 3.1**微调bert**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kgkIAXUOmvwq"
      },
      "outputs": [],
      "source": [
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\",\n",
        "    num_labels = 2, \n",
        "    output_attentions = False, \n",
        "    output_hidden_states = False,\n",
        ")\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uto80Tvhmz2I"
      },
      "outputs": [],
      "source": [
        "#设置adamW优化器\n",
        "optimizer = AdamW(\n",
        "      model.parameters(),\n",
        "      lr = 5e-5, \n",
        "      eps = 1e-8,\n",
        ")\n",
        "epochs = 4\n",
        "\n",
        "#采用交叉熵损失函数\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RyXIVxwvnBVk"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "training_stats = []\n",
        "total_t0 = time.time()\n",
        "best_accuracy = 0\n",
        "for epoch_i in range(0, epochs):\n",
        "    #训练\n",
        "    print(\"\")\n",
        "    print('Epoch {:} / {:}'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    t0 = time.time()\n",
        "    total_train_loss = 0\n",
        "    total_train_accuracy = 0\n",
        "    model.train() #将模型设置为训练模式\n",
        "    #取出一个batch的数据\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        input_ids = batch[0].to(device)\n",
        "        input_mask = batch[1].to(device)\n",
        "        labels = batch[2].to(device)\n",
        "\n",
        "        model.zero_grad()   #梯度清零 \n",
        "        out = model(input_ids, token_type_ids=None, attention_mask=input_mask, labels=labels)\n",
        "        loss = out[0]\n",
        "        logits = out[1]\n",
        "  \n",
        "        total_train_loss += loss.item()\n",
        "        loss.backward() #反向传播计算梯度 \n",
        "        # 减去大于1 的梯度，将其设为 1.0, 以防梯度爆炸.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()  #梯度更新\n",
        "\n",
        "        pred = torch.argmax(logits, dim = 1)  #取概率最大者为预测结果\n",
        "        total_train_accuracy +=  torch.sum(pred == labels).item()\n",
        "        \n",
        "    avg_train_accuracy = total_train_accuracy / len(train_dataloader.dataset) #平均训练精度\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader.dataset) #平均训练损失           \n",
        "    print(\"  Accuracy: {}\".format(avg_train_accuracy))\n",
        "    print(\"  Training loss: {}\".format(avg_train_loss))\n",
        "        \n",
        "\n",
        "    # Validation\n",
        "    print(\"\")\n",
        "    print(\"Validation...\")\n",
        "    model.eval()\n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    for batch in validation_dataloader:\n",
        "        input_ids = batch[0].to(device)\n",
        "        input_mask = batch[1].to(device)\n",
        "        labels = batch[2].to(device)\n",
        "        \n",
        "        with torch.no_grad():  #不计算梯度      \n",
        "            out = model(input_ids, token_type_ids=None, attention_mask=input_mask,labels=labels)\n",
        "            loss = out[0]\n",
        "            logits = out[1]\n",
        "\n",
        "        total_eval_loss += loss.item()\n",
        "        pred = torch.argmax(logits, dim = 1)\n",
        "        total_eval_accuracy += torch.sum(pred == labels).item()\n",
        "        y_true.append(labels.flatten())\n",
        "        y_pred.append(pred.flatten())\n",
        "        \n",
        "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader.dataset) #平均验证精度\n",
        "    print(\"  Accuracy: {}\".format(avg_val_accuracy))\n",
        "    avg_val_loss = total_eval_loss / len(validation_dataloader.dataset) #平均验证损失\n",
        "    print(\"  Validation loss: {}\".format(avg_val_loss))\n",
        "    training_time = format_time(time.time() - t0)\n",
        "    print()\n",
        "    \n",
        "    y_true = torch.cat(y_true).tolist()\n",
        "    y_pred = torch.cat(y_pred).tolist()\n",
        "    print(\"This epoch took: {:}\".format(training_time))\n",
        "    print('roc_auc score: ', roc_auc_score(y_true,y_pred))\n",
        "    print('F1 score:',f1_score(y_true, y_pred))\n",
        "    print()\n",
        "\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Train Accur.': avg_train_accuracy,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Valid. Loss': avg_val_loss,\n",
        "            'Valid. Accur.': avg_val_accuracy,\n",
        "            'Training Time': training_time,\n",
        "        }\n",
        "    )\n",
        "    print()\n",
        "\n",
        "    if avg_val_accuracy > best_accuracy:\n",
        "        best_accuracy = avg_val_accuracy\n",
        "        best_model = model\n",
        "\n",
        "print()\n",
        "print(\"=\"*10)\n",
        "print(\"Summary\")\n",
        "print(\"Total time {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X_e-ksq6nlbt"
      },
      "outputs": [],
      "source": [
        "PATH1 = \"/content/drive/MyDrive/results/'bert_finetune.pt'\"\n",
        "torch.save(model, PATH1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGZ4CSmHJLfw"
      },
      "source": [
        "### **BERT+CNN**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "prZJXEEezlCV",
        "outputId": "e19812c6-a33b-4f6b-c79a-ec668f4abc20"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-376d4aa340c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mPATH1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/drive/MyDrive/results/'bert_finetune.pt'\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mthe_best_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATH1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
          ]
        }
      ],
      "source": [
        "PATH1 = \"/content/drive/MyDrive/results/'bert_finetune.pt'\"\n",
        "the_best_model = torch.load(PATH1,map_location='cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "md3mwZb00JlR"
      },
      "outputs": [],
      "source": [
        "class BertCNNClassifier(nn.Module):\n",
        "    def __init__(self, tuned_model, embed_num = 512, embed_dim = 768, dropout=0.1, kernel_num=3, kernel_sizes=[1,2], num_labels=2):\n",
        "        super().__init__()\n",
        "        self.num_labels = num_labels\n",
        "        self.embed_num = embed_num\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dropout = dropout\n",
        "        self.kernel_num = kernel_num\n",
        "        self.kernel_sizes = kernel_sizes\n",
        "        self.softmax = nn.functional.softmax\n",
        "\n",
        "        self.bert = tuned_model.bert\n",
        "        self.convs = nn.ModuleList([nn.Conv2d(1, self.kernel_num, (k, self.embed_dim)) for k in self.kernel_sizes])\n",
        "        self.dropout = nn.Dropout(self.dropout)\n",
        "        self.classifier = nn.Linear(len(self.kernel_sizes)*self.kernel_num, self.num_labels)\n",
        "        \n",
        "    def forward(self, input_ids, attention_mask, token_type_ids = None):\n",
        "        output = self.bert(input_ids = input_ids, attention_mask = attention_mask, token_type_ids = token_type_ids) #16,512,768\n",
        "        print(\"1:\",output.shape)\n",
        "        output = output[0].unsqueeze(1) #16,1,512,768\n",
        "        print(\"2:\",-output.shape)\n",
        "        output = [nn.functional.relu(conv(output)).squeeze(3) for conv in self.convs] #16,3,508,1 => #16,3,508\n",
        "        print(\"3:\",output.shape)\n",
        "        output = [nn.functional.max_pool1d(i, i.size(2)).squeeze(2) for i in output] #=> 16,3\n",
        "        print(\"4:\",output.shape)\n",
        "        output = torch.cat(output, 1)\n",
        "        print(\"5:\",output.shape)\n",
        "        output = self.dropout(output)\n",
        "        print(\"6:\",output.shape)\n",
        "        logits = self.classifier(output)\n",
        "        print(\"7:\",output.shape)\n",
        "        return self.softmax(logits, 1)\n",
        "        print(\"8:\",output.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vfC2mlXEHl7z"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fmi0rxMl0VQ1"
      },
      "outputs": [],
      "source": [
        "# Initializing model\n",
        "model2 = BertCNNClassifier(the_best_model).cuda()\n",
        "# set parameters\n",
        "epochs = 4\n",
        "learning_rate = 5e-5\n",
        "optimizer = AdamW(model2.parameters(), lr = learning_rate)\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "howlJqMs0X8w"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "training_stats = []\n",
        "total_t0 = time.time()\n",
        "best_accuracy = 0\n",
        "for epoch_i in range(0, epochs):\n",
        "    #Training\n",
        "    print(\"\")\n",
        "    print('Epoch {:} / {:}'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    t0 = time.time()\n",
        "    total_train_loss = 0\n",
        "    total_train_accuracy = 0\n",
        "    model2.train()\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        input_ids = batch[0].to(device)\n",
        "        input_mask = batch[1].to(device)\n",
        "        labels = batch[2].to(device)\n",
        "\n",
        "        model2.zero_grad()        \n",
        "        out = model2(input_ids = input_ids, attention_mask = input_mask, token_type_ids = None)\n",
        "        loss = criterion(out, labels)\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model2.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        pred = torch.argmax(out, dim = 1)\n",
        "        total_train_accuracy +=  torch.sum(pred == labels).item()\n",
        "        \n",
        "    avg_train_accuracy = total_train_accuracy / len(train_dataloader.dataset)\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader.dataset)            \n",
        "    print(\"  Accuracy: {}\".format(avg_train_accuracy))\n",
        "    print(\"  Training loss: {}\".format(avg_train_loss))\n",
        "        \n",
        "    # Validation\n",
        "    print(\"\")\n",
        "    print(\"Validation...\")\n",
        "    model2.eval()\n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    for batch in validation_dataloader:\n",
        "        input_ids = batch[0].to(device)\n",
        "        input_mask = batch[1].to(device)\n",
        "        labels = batch[2].to(device)\n",
        "        \n",
        "        with torch.no_grad():        \n",
        "            out = model2(input_ids = input_ids, attention_mask = input_mask, token_type_ids = None)\n",
        "        loss = criterion(out, labels)\n",
        "        total_eval_loss += loss.item()\n",
        "        pred = torch.argmax(out, dim = 1)\n",
        "        total_eval_accuracy += torch.sum(pred == labels).item()\n",
        "        y_true.append(labels.flatten())\n",
        "        y_pred.append(pred.flatten())\n",
        "        \n",
        "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader.dataset)\n",
        "    print(\"  Accuracy: {}\".format(avg_val_accuracy))\n",
        "    avg_val_loss = total_eval_loss / len(validation_dataloader.dataset)\n",
        "    print(\"  Validation loss: {}\".format(avg_val_loss))\n",
        "    training_time = format_time(time.time() - t0)\n",
        "    print(\"  This epoch took: {:}\".format(training_time))\n",
        "    print()\n",
        "    y_true = torch.cat(y_true).tolist()\n",
        "    y_pred = torch.cat(y_pred).tolist()\n",
        "    print('  roc_auc score: ', roc_auc_score(y_true,y_pred))\n",
        "    print('  F1 score:',f1_score(y_true, y_pred))\n",
        "\n",
        "\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Train Accur.': avg_train_accuracy,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Valid. Loss': avg_val_loss,\n",
        "            'Valid. Accur.': avg_val_accuracy,\n",
        "            'Training Time': training_time,\n",
        "        }\n",
        "    )\n",
        "\n",
        "    if avg_val_accuracy > best_accuracy:\n",
        "        best_accuracy = avg_val_accuracy\n",
        "        best_model = model2\n",
        "\n",
        "print(\"===\")\n",
        "print(\"Summary\")\n",
        "print(\"Total time {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n",
        "print('best acc:',best_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0bt1oghMNGu"
      },
      "source": [
        "# 3.2.2 **固定模型参数**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vI0cwQn-0avZ"
      },
      "outputs": [],
      "source": [
        "class BertCNNClassifier(nn.Module):\n",
        "    def __init__(self, tuned_model, embed_num = 512, embed_dim = 768, dropout=0.1, kernel_num=3, kernel_sizes=[1,2], num_labels=2):\n",
        "        super().__init__()\n",
        "        self.num_labels = num_labels\n",
        "        self.embed_num = embed_num\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dropout = dropout\n",
        "        self.kernel_num = kernel_num\n",
        "        self.kernel_sizes = kernel_sizes\n",
        "        self.softmax = nn.functional.softmax\n",
        "\n",
        "        self.bert = tuned_model.bert\n",
        "        self.convs = nn.ModuleList([nn.Conv2d(1, self.kernel_num, (k, self.embed_dim)) for k in self.kernel_sizes])\n",
        "        self.dropout = nn.Dropout(self.dropout)\n",
        "        self.classifier = nn.Linear(len(self.kernel_sizes)*self.kernel_num, self.num_labels)\n",
        "        \n",
        "    def forward(self, input_ids, attention_mask, token_type_ids = None):\n",
        "        output = self.bert(input_ids = input_ids, attention_mask = attention_mask, token_type_ids = token_type_ids) #16,512,768\n",
        "        output = output[0].unsqueeze(1) #16,1,512,768\n",
        "        output = [nn.functional.relu(conv(output)).squeeze(3) for conv in self.convs] #16,3,508,1 => #16,3,508\n",
        "        output = [nn.functional.max_pool1d(i, i.size(2)).squeeze(2) for i in output] #=> 16,3\n",
        "        output = torch.cat(output, 1)\n",
        "        output = self.dropout(output)\n",
        "        logits = self.classifier(output)\n",
        "        return self.softmax(logits, 1)\n",
        "      "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qnxUm4oEMt23"
      },
      "outputs": [],
      "source": [
        "# Initializing model\n",
        "model3 = BertCNNClassifier(the_best_model).cuda()\n",
        "for param in model3.bert.parameters():\n",
        "    param.requires_grad = False\n",
        "# set parameters\n",
        "epochs = 4\n",
        "learning_rate = 5e-5\n",
        "optimizer = AdamW(model3.parameters(), lr = learning_rate)\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mGDY0g5xHCDc"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "training_stats = []\n",
        "total_t0 = time.time()\n",
        "best_accuracy = 0\n",
        "for epoch_i in range(0, epochs):\n",
        "    #Training\n",
        "    print(\"\")\n",
        "    print('Epoch {:} / {:}'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    t0 = time.time()\n",
        "    total_train_loss = 0\n",
        "    total_train_accuracy = 0\n",
        "    model3.train()\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        input_ids = batch[0].to(device)\n",
        "        input_mask = batch[1].to(device)\n",
        "        labels = batch[2].to(device)\n",
        "\n",
        "        model3.zero_grad()        \n",
        "        out = model3(input_ids = input_ids, attention_mask = input_mask, token_type_ids = None)\n",
        "        loss = criterion(out, labels)\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model3.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        pred = torch.argmax(out, dim = 1)\n",
        "        total_train_accuracy +=  torch.sum(pred == labels).item()\n",
        "        \n",
        "    avg_train_accuracy = total_train_accuracy / len(train_dataloader.dataset)\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader.dataset)            \n",
        "    print(\"  Accuracy: {}\".format(avg_train_accuracy))\n",
        "    print(\"  Training loss: {}\".format(avg_train_loss))\n",
        "        \n",
        "    # Validation\n",
        "    print(\"\")\n",
        "    print(\"Validation...\")\n",
        "    model3.eval()\n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    for batch in validation_dataloader:\n",
        "        input_ids = batch[0].to(device)\n",
        "        input_mask = batch[1].to(device)\n",
        "        labels = batch[2].to(device)\n",
        "        \n",
        "        with torch.no_grad():        \n",
        "            out = model3(input_ids = input_ids, attention_mask = input_mask, token_type_ids = None)\n",
        "        loss = criterion(out, labels)\n",
        "        total_eval_loss += loss.item()\n",
        "        pred = torch.argmax(out, dim = 1)\n",
        "        total_eval_accuracy += torch.sum(pred == labels).item()\n",
        "        y_true.append(labels.flatten())\n",
        "        y_pred.append(pred.flatten())\n",
        "        \n",
        "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader.dataset)\n",
        "    print(\"  Accuracy: {}\".format(avg_val_accuracy))\n",
        "    avg_val_loss = total_eval_loss / len(validation_dataloader.dataset)\n",
        "    print(\"  Validation loss: {}\".format(avg_val_loss))\n",
        "    training_time = format_time(time.time() - t0)\n",
        "    print(\"  This epoch took: {:}\".format(training_time))\n",
        "    print()\n",
        "    y_true = torch.cat(y_true).tolist()\n",
        "    y_pred = torch.cat(y_pred).tolist()\n",
        "    print('  roc_auc score: ', roc_auc_score(y_true,y_pred))\n",
        "    print('  F1 score:',f1_score(y_true, y_pred))\n",
        "\n",
        "\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Train Accur.': avg_train_accuracy,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Valid. Loss': avg_val_loss,\n",
        "            'Valid. Accur.': avg_val_accuracy,\n",
        "            'Training Time': training_time,\n",
        "        }\n",
        "    )\n",
        "\n",
        "    if avg_val_accuracy > best_accuracy:\n",
        "        best_accuracy = avg_val_accuracy\n",
        "        best_model = model3\n",
        "\n",
        "print(\"===\")\n",
        "print(\"Summary\")\n",
        "print(\"Total time {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n",
        "print('best acc:',best_accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qOHm1IAIHFKD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRa0prizM455"
      },
      "source": [
        "# 3.3BERT+**LSTM**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWNxzY2fOzkp"
      },
      "source": [
        "\n",
        "3.3.1 Without Freeze Parameters¶"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rB4hz0RjO0E7"
      },
      "outputs": [],
      "source": [
        "class BertLstmClassifier(nn.Module):\n",
        "    def __init__(self, model_tune):\n",
        "        super().__init__()\n",
        "        self.bert = model_tune.bert\n",
        "        self.lstm = nn.LSTM(input_size = 768, \n",
        "                            hidden_size = 768, \n",
        "                            num_layers = 1, \n",
        "                            batch_first = True, \n",
        "                            bidirectional = True)\n",
        "        self.classifier = nn.Linear(768 * 2, 2)\n",
        "        self.softmax = nn.Softmax(dim = 1)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
        "        bert_output = self.bert(input_ids = input_ids, attention_mask = attention_mask, token_type_ids = token_type_ids)\n",
        "        out, _ = self.lstm(bert_output[0])\n",
        "        logits = self.classifier(out[:, 1, :])\n",
        "        return self.softmax(logits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wD9I7z6lO3NV"
      },
      "outputs": [],
      "source": [
        "# Initializing model\n",
        "model4 = BertLstmClassifier(the_best_model).cuda()\n",
        "# set parameters\n",
        "epochs = 6\n",
        "learning_rate = 5e-5\n",
        "optimizer = AdamW(model4.parameters(), lr = learning_rate)\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VfiDnbipPALM"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "training_stats = []\n",
        "total_t0 = time.time()\n",
        "best_accuracy = 0\n",
        "for epoch_i in range(0, epochs):\n",
        "    #Training\n",
        "    print(\"\")\n",
        "    print('Epoch {:} / {:}'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    t0 = time.time()\n",
        "    total_train_loss = 0\n",
        "    total_train_accuracy = 0\n",
        "    model4.train()\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        input_ids = batch[0].to(device)\n",
        "        input_mask = batch[1].to(device)\n",
        "        labels = batch[2].to(device)\n",
        "\n",
        "        model4.zero_grad()        \n",
        "        out = model4(input_ids = input_ids, attention_mask = input_mask, token_type_ids = None)\n",
        "        loss = criterion(out, labels)\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model4.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        pred = torch.argmax(out, dim = 1)\n",
        "        total_train_accuracy +=  torch.sum(pred == labels).item()\n",
        "        \n",
        "    avg_train_accuracy = total_train_accuracy / len(train_dataloader.dataset)\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader.dataset)            \n",
        "    print(\"  Accuracy: {}\".format(avg_train_accuracy))\n",
        "    print(\"  Training loss: {}\".format(avg_train_loss))\n",
        "        \n",
        "    # Validation\n",
        "    print(\"\")\n",
        "    print(\"Validation...\")\n",
        "    model4.eval()\n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    for batch in validation_dataloader:\n",
        "        input_ids = batch[0].to(device)\n",
        "        input_mask = batch[1].to(device)\n",
        "        labels = batch[2].to(device)\n",
        "        \n",
        "        with torch.no_grad():        \n",
        "            out = model4(input_ids = input_ids, attention_mask = input_mask, token_type_ids = None)\n",
        "        loss = criterion(out, labels)\n",
        "        total_eval_loss += loss.item()\n",
        "        pred = torch.argmax(out, dim = 1)\n",
        "        total_eval_accuracy += torch.sum(pred == labels).item()\n",
        "        y_true.append(labels.flatten())\n",
        "        y_pred.append(pred.flatten())\n",
        "        \n",
        "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader.dataset)\n",
        "    print(\"  Accuracy: {}\".format(avg_val_accuracy))\n",
        "    avg_val_loss = total_eval_loss / len(validation_dataloader.dataset)\n",
        "    print(\"  Validation loss: {}\".format(avg_val_loss))\n",
        "    training_time = format_time(time.time() - t0)\n",
        "    print(\"  This epoch took: {:}\".format(training_time))\n",
        "    print()\n",
        "    y_true = torch.cat(y_true).tolist()\n",
        "    y_pred = torch.cat(y_pred).tolist()\n",
        "    print('  roc_auc score: ', roc_auc_score(y_true,y_pred))\n",
        "    print('  F1 score:',f1_score(y_true, y_pred))\n",
        "\n",
        "\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Train Accur.': avg_train_accuracy,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Valid. Loss': avg_val_loss,\n",
        "            'Valid. Accur.': avg_val_accuracy,\n",
        "            'Training Time': training_time,\n",
        "        }\n",
        "    )\n",
        "\n",
        "    if avg_val_accuracy > best_accuracy:\n",
        "        best_accuracy = avg_val_accuracy\n",
        "        best_model = model4\n",
        "\n",
        "print(\"===\")\n",
        "print(\"Summary\")\n",
        "print(\"Total time {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n",
        "print('best acc:',best_accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ZNj5s-oPECZ"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qh7lWDq4KUgd"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
